{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q:\\Projects\\Multimodal-Jarvis\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import gradio as gr\n",
    "from time import sleep\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 17:09:20,995 - INFO - Logging is set up!\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Change to DEBUG for more details\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(r\"Q:\\Projects\\Multimodal-Jarvis\\data\\logs\\app.log\"),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Show logs in the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"Logging is set up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# https://www.gradio.app/guides/theming-guide\n",
    "# https://huggingface.co/spaces/gstaff/xkcd/blob/main/app.py\n",
    "# test_theme = gr.Theme.from_hub(\"gstaff/xkcd\")\n",
    "# gr.themes.builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTabItem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvisible\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minteractive\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0melem_id\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0melem_classes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list[str] | str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mscale\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Tab (or its alias TabItem) is a layout element. Components defined within the Tab will be visible when this tab is selected tab.\n",
      "Example:\n",
      "    with gr.Blocks() as demo:\n",
      "        with gr.Tab(\"Lion\"):\n",
      "            gr.Image(\"lion.jpg\")\n",
      "            gr.Button(\"New Lion\")\n",
      "        with gr.Tab(\"Tiger\"):\n",
      "            gr.Image(\"tiger.jpg\")\n",
      "            gr.Button(\"New Tiger\")\n",
      "Guides: controlling-layout\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Parameters:\n",
      "    label: The visual label for the tab\n",
      "    id: An optional identifier for the tab, required if you wish to control the selected tab from a predict function.\n",
      "    elem_id: An optional string that is assigned as the id of the <div> containing the contents of the Tab layout. The same string followed by \"-button\" is attached to the Tab button. Can be used for targeting CSS styles.\n",
      "    elem_classes: An optional string or list of strings that are assigned as the class of this component in the HTML DOM. Can be used for targeting CSS styles.\n",
      "    render: If False, this layout will not be rendered in the Blocks context. Should be used if the intention is to assign event listeners now but render the component later.\n",
      "    scale: relative size compared to adjacent elements. 1 or greater indicates the Tab will expand in size.\n",
      "    visible: If False, Tab will be hidden.\n",
      "    interactive: If False, Tab will not be clickable.\n",
      "\u001b[1;31mFile:\u001b[0m           q:\\minianaconda\\envs\\ml\\lib\\site-packages\\gradio\\layouts\\tabs.py\n",
      "\u001b[1;31mType:\u001b[0m           ComponentMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "gr.TabItem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = gr.themes.Default(\n",
    "    font=['Noto Sans', 'Helvetica', 'ui-sans-serif', 'system-ui', 'sans-serif'],\n",
    "    font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],\n",
    ").set(\n",
    "    border_color_primary='#c5c5d2',\n",
    "    button_large_padding='6px 12px',\n",
    "    body_text_color_subdued='#484848',\n",
    "    background_fill_secondary='#eaeaea',\n",
    "    background_fill_primary='var(--neutral-50)',\n",
    "    body_background_fill=\"white\",\n",
    "    block_background_fill=\"#f4f4f4\",\n",
    "    body_text_color=\"#333\",\n",
    "    button_secondary_background_fill=\"#f4f4f4\",\n",
    "    button_secondary_border_color=\"var(--border-color-primary)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "Device set to use cpu\n",
      "2025-02-03 16:19:13,778 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-02-03 16:19:13,802 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:19:14,122 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/initiated HTTP/1.1\" 200 0\n",
      "2025-02-03 16:19:14,122 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002170CD5EF50>\n",
      "2025-02-03 16:19:14,232 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002170CC2C640> server_hostname='api.gradio.app' timeout=3\n",
      "2025-02-03 16:19:14,441 - DEBUG - Using selector: SelectSelector\n",
      "2025-02-03 16:19:14,465 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None\n",
      "2025-02-03 16:19:14,467 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002170CFB1000>\n",
      "2025-02-03 16:19:14,468 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:14,472 - DEBUG - send_request_headers.complete\n",
      "2025-02-03 16:19:14,473 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:14,474 - DEBUG - send_request_body.complete\n",
      "2025-02-03 16:19:14,475 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:14,477 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Mon, 03 Feb 2025 14:19:14 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])\n",
      "2025-02-03 16:19:14,478 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 16:19:14,480 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:14,482 - DEBUG - receive_response_body.complete\n",
      "2025-02-03 16:19:14,483 - DEBUG - response_closed.started\n",
      "2025-02-03 16:19:14,484 - DEBUG - response_closed.complete\n",
      "2025-02-03 16:19:14,486 - DEBUG - close.started\n",
      "2025-02-03 16:19:14,488 - DEBUG - close.complete\n",
      "2025-02-03 16:19:14,490 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None\n",
      "2025-02-03 16:19:14,493 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002170CFB2200>\n",
      "2025-02-03 16:19:14,495 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>\n",
      "2025-02-03 16:19:14,497 - DEBUG - send_request_headers.complete\n",
      "2025-02-03 16:19:14,498 - DEBUG - send_request_body.started request=<Request [b'HEAD']>\n",
      "2025-02-03 16:19:14,502 - DEBUG - send_request_body.complete\n",
      "2025-02-03 16:19:14,515 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>\n",
      "2025-02-03 16:19:14,519 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Mon, 03 Feb 2025 14:19:14 GMT'), (b'server', b'uvicorn'), (b'content-length', b'72714'), (b'content-type', b'text/html; charset=utf-8')])\n",
      "2025-02-03 16:19:14,522 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 16:19:14,523 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>\n",
      "2025-02-03 16:19:14,525 - DEBUG - receive_response_body.complete\n",
      "2025-02-03 16:19:14,527 - DEBUG - response_closed.started\n",
      "2025-02-03 16:19:14,528 - DEBUG - response_closed.complete\n",
      "2025-02-03 16:19:14,530 - DEBUG - close.started\n",
      "2025-02-03 16:19:14,533 - DEBUG - close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "Monitoring URL: http://127.0.0.1:7860/monitoring/wQA_QSM6iPiclgqjEEHN2w\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:19:14,540 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-02-03 16:19:14,673 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002170CC611E0>\n",
      "2025-02-03 16:19:14,675 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:14,677 - DEBUG - send_request_headers.complete\n",
      "2025-02-03 16:19:14,678 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:14,679 - DEBUG - send_request_body.complete\n",
      "2025-02-03 16:19:14,682 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:14,755 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/launched HTTP/1.1\" 200 0\n",
      "2025-02-03 16:19:14,887 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 03 Feb 2025 14:19:15 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "2025-02-03 16:19:14,889 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 16:19:14,891 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:14,892 - DEBUG - receive_response_body.complete\n",
      "2025-02-03 16:19:14,892 - DEBUG - response_closed.started\n",
      "2025-02-03 16:19:14,894 - DEBUG - response_closed.complete\n",
      "2025-02-03 16:19:14,895 - DEBUG - close.started\n",
      "2025-02-03 16:19:14,896 - DEBUG - close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "def enable_input():\n",
    "    return gr.update(interactive=True)\n",
    "\n",
    "def print_like_dislike(x: gr.LikeData):\n",
    "    print(x.index, x.value, x.liked)\n",
    "\n",
    "def handle_undo(history: list[gr.ChatMessage], undo_data: gr.UndoData) -> tuple[list[gr.ChatMessage], str]:\n",
    "    return history[:undo_data.index], history[undo_data.index].content \n",
    "\n",
    "\n",
    "def handle_edit(history: list[gr.ChatMessage], edit_data: gr.EditData) -> list[gr.ChatMessage]:\n",
    "    new_history = history[:edit_data.index]\n",
    "    new_history[-1].content = edit_data.value  \n",
    "    return new_history\n",
    "\n",
    "\n",
    "def add_message(history, message):\n",
    "    if message[\"text\"] is not None:\n",
    "      history.append(gr.ChatMessage(role = \"user\", content = message[\"text\"])) \n",
    "\n",
    "    for file_path in message[\"files\"]:\n",
    "        if file_path.endswith(\".wav\"):\n",
    "            transcribed_text = audiofile_to_text(file_path)\n",
    "            history.append(gr.ChatMessage(role = \"user\", content = transcribed_text))\n",
    "    return history, gr.MultimodalTextbox(value=None, interactive=False)\n",
    "\n",
    "def bot_output(history: list):\n",
    "    try:\n",
    "        history.append(gr.ChatMessage(role=\"assistant\", content=\"\"))\n",
    "        \n",
    "        text = tokenizer.apply_chat_template(\n",
    "            history,    # [msg.dict() for msg in history]\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        model_inputs  = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        generated_text  = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        if not generated_text or not isinstance(generated_text, str):  \n",
    "            generated_text = \"I'm sorry, but I couldn't generate a response.\"\n",
    "\n",
    "        for char in generated_text :\n",
    "            history[-1].content += char\n",
    "            sleep(0.05) \n",
    "            yield history\n",
    "\n",
    "        history = text_to_audiofile(generated_text, history)\n",
    "        yield history\n",
    "    except Exception as e:\n",
    "        history.append(gr.ChatMessage(role=\"system\", content=f\"Failed to create text: {str(e)}\"))\n",
    "        yield history\n",
    "        raise gr.Error(f\"Failed to create text: {str(e)}\")\n",
    "\n",
    "def audiofile_to_text(wav_path):\n",
    "    try:\n",
    "        sample_rate, audio_data = wavfile.read(wav_path)\n",
    "        audio_data = np.array(audio_data, dtype=np.float32)\n",
    "        audio_data /= np.max(np.abs(audio_data))\n",
    "        \n",
    "        transcribed_text = transcriber_model({\"raw\": audio_data, \"sampling_rate\": sample_rate})[\"text\"]\n",
    "        return str(transcribed_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise gr.Error(f\"Failed to transcribe audio: {e}\")\n",
    "\n",
    "def text_to_audiofile(input_text, history):\n",
    "    try:\n",
    "        speech = synthesiser(input_text, forward_params = {\"do_sample\": True})\n",
    "        rate_speech = speech[\"sampling_rate\"]\n",
    "        data_speech = speech[\"audio\"]\n",
    "        data_speech = data_speech.flatten()\n",
    "        data_speech = np.int16(data_speech / np.max(np.abs(data_speech)) * 32767)\n",
    "\n",
    "        wavfile.write(r\"Q:\\Projects\\Multimodal-Jarvis\\data\\audio\\bark_out.wav\", rate=rate_speech, data=data_speech)\n",
    "        history.append(gr.ChatMessage(role=\"assistant\", content= \n",
    "            gr.Audio(r\"Q:\\Projects\\Multimodal-Jarvis\\data\\audio\\bark_out.wav\"),\n",
    "            metadata={\"title\": rf\"üõ†Ô∏è Used tool {model_name_tts}\"}))\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        raise gr.Error(f\"Failed to convert text to audio: {e}\")\n",
    "\n",
    "model_name_nlp = r\"Q:\\Projects\\Multimodal-Jarvis\\models\\nlp\\Qwen2.5-1.5B-Instruct\"\n",
    "model_name_stt = r\"Q:\\Projects\\Multimodal-Jarvis\\models\\stt\\whisper-large-v3-turbo\"\n",
    "model_name_tts = r\"Q:\\Projects\\Multimodal-Jarvis\\models\\tts\\Suno-Bark\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_nlp)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_nlp, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "transcriber_model = pipeline(\"automatic-speech-recognition\", model = model_name_stt)\n",
    "synthesiser = pipeline(\"text-to-speech\", model = model_name_tts)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for inference\")\n",
    "\n",
    "\n",
    "# callback = gr.CSVLogger()\n",
    "def bot_ui():\n",
    "    with gr.Blocks(theme=theme) as blocks:\n",
    "        blocks.analytics_enabled = True\n",
    "        gr.Markdown(\n",
    "        f\"\"\"\n",
    "        # {\" \".join(os.path.basename(model_name_nlp).split(\"-\"))} Test\n",
    "        \"\"\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=10):\n",
    "                chatbot = gr.Chatbot(elem_id=\"chatbot\", \n",
    "                    height=500,\n",
    "                    type=\"messages\",\n",
    "                    bubble_full_width=False,\n",
    "                    placeholder=f\"<strong><br><big>JARvis</strong>\",\n",
    "                    editable=True\n",
    "                )\n",
    "\n",
    "                chat_input  = gr.MultimodalTextbox(\n",
    "                    interactive=True,\n",
    "                    file_count=\"multiple\",\n",
    "                    placeholder=\"Ask me a question\",\n",
    "                    container=False,\n",
    "                    show_label=False,\n",
    "                    sources=[\"microphone\", \"upload\"],\n",
    "                )\n",
    "\n",
    "                chat_msg = chat_input.submit(\n",
    "                    add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
    "                )\n",
    "                # callback.setup([chat_msg, chat_input], \"chat_messages.csv\")\n",
    "\n",
    "                bot_msg = chat_msg.then(bot_output, chatbot, chatbot, api_name=\"bot_response\")\n",
    "                bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input], concurrency_limit = 40)\n",
    "\n",
    "                chatbot.like(print_like_dislike, None, None)\n",
    "                chatbot.edit(handle_edit, chatbot, chatbot)\n",
    "                chatbot.undo(handle_undo, chatbot, [chatbot, chat_input])\n",
    "    return blocks\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = bot_ui()\n",
    "    demo.queue(api_open=False)\n",
    "    demo.launch(show_error=True, \n",
    "                show_api=True, \n",
    "                debug=True, \n",
    "                allowed_paths = [r\"Q:\\Projects\\Multimodal-Jarvis\\data\\audio\"],\n",
    "                enable_monitoring=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 17:50:16,047 - WARNING - Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "Device set to use cpu\n",
      "q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "Device set to use cpu\n",
      "2025-02-06 17:50:19,086 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-02-06 17:50:19,130 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
      "2025-02-06 17:50:19,256 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 17:50:19,270 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
      "2025-02-06 17:50:19,549 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/initiated HTTP/1.1\" 200 0\n",
      "2025-02-06 17:50:19,581 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001515E8605B0>\n",
      "2025-02-06 17:50:19,613 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001515E71A540> server_hostname='api.gradio.app' timeout=3\n",
      "2025-02-06 17:50:19,673 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001515E7033A0>\n",
      "2025-02-06 17:50:19,709 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001515E719840> server_hostname='api.gradio.app' timeout=3\n",
      "2025-02-06 17:50:19,772 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/initiated HTTP/1.1\" 200 0\n",
      "2025-02-06 17:50:20,076 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000151603A3C40>\n",
      "2025-02-06 17:50:20,083 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:20,087 - DEBUG - send_request_headers.complete\n",
      "2025-02-06 17:50:20,093 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:20,093 - DEBUG - send_request_body.complete\n",
      "2025-02-06 17:50:20,105 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:20,223 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001515E703CD0>\n",
      "2025-02-06 17:50:20,223 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:20,235 - DEBUG - send_request_headers.complete\n",
      "2025-02-06 17:50:20,238 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:20,245 - DEBUG - send_request_body.complete\n",
      "2025-02-06 17:50:20,246 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:20,303 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Feb 2025 15:50:20 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "2025-02-06 17:50:20,316 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-02-06 17:50:20,321 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:20,327 - DEBUG - receive_response_body.complete\n",
      "2025-02-06 17:50:20,338 - DEBUG - response_closed.started\n",
      "2025-02-06 17:50:20,343 - DEBUG - response_closed.complete\n",
      "2025-02-06 17:50:20,351 - DEBUG - close.started\n",
      "2025-02-06 17:50:20,359 - DEBUG - close.complete\n",
      "2025-02-06 17:50:20,453 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Feb 2025 15:50:20 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "2025-02-06 17:50:20,453 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-02-06 17:50:20,462 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:20,463 - DEBUG - receive_response_body.complete\n",
      "2025-02-06 17:50:20,469 - DEBUG - response_closed.started\n",
      "2025-02-06 17:50:20,473 - DEBUG - response_closed.complete\n",
      "2025-02-06 17:50:20,480 - DEBUG - close.started\n",
      "2025-02-06 17:50:20,487 - DEBUG - close.complete\n",
      "2025-02-06 17:50:21,001 - DEBUG - Using selector: SelectSelector\n",
      "2025-02-06 17:50:21,024 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None\n",
      "2025-02-06 17:50:21,028 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001515EF9A230>\n",
      "2025-02-06 17:50:21,029 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:21,030 - DEBUG - send_request_headers.complete\n",
      "2025-02-06 17:50:21,032 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:21,033 - DEBUG - send_request_body.complete\n",
      "2025-02-06 17:50:21,034 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:21,035 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 06 Feb 2025 15:50:21 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])\n",
      "2025-02-06 17:50:21,037 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-02-06 17:50:21,038 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-02-06 17:50:21,039 - DEBUG - receive_response_body.complete\n",
      "2025-02-06 17:50:21,040 - DEBUG - response_closed.started\n",
      "2025-02-06 17:50:21,040 - DEBUG - response_closed.complete\n",
      "2025-02-06 17:50:21,042 - DEBUG - close.started\n",
      "2025-02-06 17:50:21,044 - DEBUG - close.complete\n",
      "2025-02-06 17:50:21,046 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None\n",
      "2025-02-06 17:50:21,049 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001515F9F81C0>\n",
      "2025-02-06 17:50:21,051 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>\n",
      "2025-02-06 17:50:21,053 - DEBUG - send_request_headers.complete\n",
      "2025-02-06 17:50:21,056 - DEBUG - send_request_body.started request=<Request [b'HEAD']>\n",
      "2025-02-06 17:50:21,071 - DEBUG - send_request_body.complete\n",
      "2025-02-06 17:50:21,072 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>\n",
      "2025-02-06 17:50:21,074 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 06 Feb 2025 15:50:21 GMT'), (b'server', b'uvicorn'), (b'content-length', b'59319'), (b'content-type', b'text/html; charset=utf-8')])\n",
      "2025-02-06 17:50:21,075 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "2025-02-06 17:50:21,076 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>\n",
      "2025-02-06 17:50:21,078 - DEBUG - receive_response_body.complete\n",
      "2025-02-06 17:50:21,079 - DEBUG - response_closed.started\n",
      "2025-02-06 17:50:21,080 - DEBUG - response_closed.complete\n",
      "2025-02-06 17:50:21,081 - DEBUG - close.started\n",
      "2025-02-06 17:50:21,083 - DEBUG - close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "Monitoring URL: http://127.0.0.1:7860/monitoring/RLPB9INg5i1V1ffxtrfBRw\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 17:50:21,090 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-02-06 17:50:21,394 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/launched HTTP/1.1\" 200 0\n",
      "2025-02-06 17:50:31,918 - DEBUG - Calling on_part_begin with no data\n",
      "2025-02-06 17:50:31,919 - DEBUG - Calling on_header_field with data[42:61]\n",
      "2025-02-06 17:50:31,925 - DEBUG - Calling on_header_value with data[63:108]\n",
      "2025-02-06 17:50:31,926 - DEBUG - Calling on_header_end with no data\n",
      "2025-02-06 17:50:31,928 - DEBUG - Calling on_header_field with data[110:122]\n",
      "2025-02-06 17:50:31,929 - DEBUG - Calling on_header_value with data[124:148]\n",
      "2025-02-06 17:50:31,931 - DEBUG - Calling on_header_end with no data\n",
      "2025-02-06 17:50:31,933 - DEBUG - Calling on_headers_finished with no data\n",
      "2025-02-06 17:50:31,938 - DEBUG - Calling on_part_data with data[152:212992]\n",
      "2025-02-06 17:50:31,943 - DEBUG - Calling on_part_data with data[0:30634]\n",
      "2025-02-06 17:50:31,945 - DEBUG - Calling on_part_end with no data\n",
      "2025-02-06 17:50:31,947 - DEBUG - Calling on_end with no data\n",
      "2025-02-06 17:50:31,978 - DEBUG - Calling on_part_begin with no data\n",
      "2025-02-06 17:50:31,980 - DEBUG - Calling on_header_field with data[42:61]\n",
      "2025-02-06 17:50:31,981 - DEBUG - Calling on_header_value with data[63:108]\n",
      "2025-02-06 17:50:31,983 - DEBUG - Calling on_header_end with no data\n",
      "2025-02-06 17:50:31,984 - DEBUG - Calling on_header_field with data[110:122]\n",
      "2025-02-06 17:50:31,985 - DEBUG - Calling on_header_value with data[124:148]\n",
      "2025-02-06 17:50:31,987 - DEBUG - Calling on_header_end with no data\n",
      "2025-02-06 17:50:31,989 - DEBUG - Calling on_headers_finished with no data\n",
      "2025-02-06 17:50:31,991 - DEBUG - Calling on_part_data with data[152:243626]\n",
      "2025-02-06 17:50:31,991 - DEBUG - Calling on_part_end with no data\n",
      "2025-02-06 17:50:31,993 - DEBUG - Calling on_end with no data\n",
      "q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 324, in jsonable_encoder\n",
      "    data = dict(obj)\n",
      "TypeError: cannot convert dictionary update sequence element #0 to a sequence\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 329, in jsonable_encoder\n",
      "    data = vars(obj)\n",
      "TypeError: vars() argument must have __dict__ attribute\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\route_utils.py\", line 795, in __call__\n",
      "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\route_utils.py\", line 811, in simple_response\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\routing.py\", line 735, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\starlette\\routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\routing.py\", line 327, in app\n",
      "    content = await serialize_response(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\routing.py\", line 201, in serialize_response\n",
      "    return jsonable_encoder(response_content)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 289, in jsonable_encoder\n",
      "    encoded_value = jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 303, in jsonable_encoder\n",
      "    jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 303, in jsonable_encoder\n",
      "    jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 303, in jsonable_encoder\n",
      "    jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 289, in jsonable_encoder\n",
      "    encoded_value = jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 333, in jsonable_encoder\n",
      "    return jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 289, in jsonable_encoder\n",
      "    encoded_value = jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 303, in jsonable_encoder\n",
      "    jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 289, in jsonable_encoder\n",
      "    encoded_value = jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 303, in jsonable_encoder\n",
      "    jsonable_encoder(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\fastapi\\encoders.py\", line 332, in jsonable_encoder\n",
      "    raise ValueError(errors) from e\n",
      "ValueError: [TypeError('cannot convert dictionary update sequence element #0 to a sequence'), TypeError('vars() argument must have __dict__ attribute')]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11888\\2847510476.py\", line 72, in bot_output\n",
      "    text = tokenizer.apply_chat_template(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 1683, in apply_chat_template\n",
      "    rendered_chat = compiled_template.render(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\jinja2\\environment.py\", line 1295, in render\n",
      "    self.environment.handle_exception()\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\jinja2\\environment.py\", line 942, in handle_exception\n",
      "    raise rewrite_traceback_stack(source=source)\n",
      "  File \"<template>\", line 23, in top-level template code\n",
      "TypeError: can only concatenate str (not \"tuple\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\blocks.py\", line 2044, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\blocks.py\", line 1603, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py\", line 833, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\chat_interface.py\", line 898, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py\", line 722, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py\", line 705, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11888\\2847510476.py\", line 95, in bot_output\n",
      "    raise gr.Error(f\"Failed to create text: {str(e)}\")\n",
      "gradio.exceptions.Error: 'Failed to create text: can only concatenate str (not \"tuple\") to str'\n",
      "Traceback (most recent call last):\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\routes.py\", line 993, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\blocks.py\", line 2044, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\blocks.py\", line 1591, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py\", line 883, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\chat_interface.py\", line 468, in _save_conversation\n",
      "    saved_conversations[index] = conversation\n",
      "IndexError: list assignment index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import gradio as gr\n",
    "from time import sleep\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io import wavfile\n",
    "\n",
    "model_name_nlp = r\"Q:\\Projects\\Multimodal-Jarvis\\models\\nlp\\Qwen2.5-1.5B-Instruct\"\n",
    "model_name_stt = r\"Q:\\Projects\\Multimodal-Jarvis\\models\\stt\\whisper-large-v3-turbo\"\n",
    "model_name_tts = r\"Q:\\Projects\\Multimodal-Jarvis\\models\\tts\\Suno-Bark\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_nlp)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_nlp,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "transcriber_model = pipeline(\"automatic-speech-recognition\", model = model_name_stt)\n",
    "synthesiser = pipeline(\"text-to-speech\", model = model_name_tts)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for inference\")\n",
    "\n",
    "def audiofile_to_text(wav_path):\n",
    "    try:\n",
    "        sample_rate, audio_data = wavfile.read(wav_path)\n",
    "        audio_data = np.array(audio_data, dtype=np.float32)\n",
    "        audio_data /= np.max(np.abs(audio_data))\n",
    "        \n",
    "        transcribed_text = transcriber_model({\"raw\": audio_data, \"sampling_rate\": sample_rate})[\"text\"]\n",
    "        return str(transcribed_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise gr.Error(f\"Failed to transcribe audio: {e}\")\n",
    "\n",
    "def text_to_audiofile(input_text, history):\n",
    "    try:\n",
    "        speech = synthesiser(input_text, forward_params = {\"do_sample\": True})\n",
    "        rate_speech = speech[\"sampling_rate\"]\n",
    "        data_speech = speech[\"audio\"]\n",
    "        data_speech = data_speech.flatten()\n",
    "        data_speech = np.int16(data_speech / np.max(np.abs(data_speech)) * 32767)\n",
    "\n",
    "        wavfile.write(r\"Q:\\Projects\\Multimodal-Jarvis\\data\\audio\\bark_out.wav\", rate=rate_speech, data=data_speech)\n",
    "        history.append(gr.ChatMessage(role=\"assistant\", content= \n",
    "            gr.Audio(r\"Q:\\Projects\\Multimodal-Jarvis\\data\\audio\\bark_out.wav\"),\n",
    "            metadata={\"title\": rf\"üõ†Ô∏è Used tool {model_name_tts}\"}))\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        raise gr.Error(f\"Failed to convert text to audio: {e}\")\n",
    "\n",
    "\n",
    "def bot_output(message, history: list):\n",
    "    try:\n",
    "        output_history = [] # append transcribed text to output_history\n",
    "\n",
    "        for file_path in message[\"files\"]:\n",
    "            try:\n",
    "                transcribed_text = audiofile_to_text(file_path)\n",
    "                history.append(gr.ChatMessage(role=\"user\", content=transcribed_text))\n",
    "                output_history.append(gr.ChatMessage(role=\"assistant\", content=f\"**{transcribed_text}**\"))\n",
    "\n",
    "            except Exception as transcription_error:\n",
    "                print(f\"Error transcribing {file_path}: {transcription_error}\")\n",
    "                history.append(gr.ChatMessage(role=\"assistant\", \n",
    "                                                content=f\"Failed to transcribe {file_path}\"))\n",
    "           \n",
    "        output_history.append(gr.ChatMessage(role=\"assistant\", content=\"\"))\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            history,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        model_inputs  = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        generated_text  = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        if not generated_text or not isinstance(generated_text, str):  \n",
    "            generated_text = \"I'm sorry, but I couldn't generate a response.\"\n",
    "\n",
    "        for char in generated_text :\n",
    "            output_history[-1].content += char\n",
    "            sleep(0.01) \n",
    "            yield output_history\n",
    "\n",
    "        output_history = text_to_audiofile(generated_text, output_history)\n",
    "        yield output_history\n",
    "\n",
    "    except Exception as e:\n",
    "        raise gr.Error(f\"Failed to create text: {str(e)}\")\n",
    "\n",
    "def chat_ui():\n",
    "    with gr.Blocks(theme=theme) as blocks:\n",
    "        gr.Markdown(\n",
    "            f\"\"\"\n",
    "            # {\" \".join(os.path.basename(model_name_nlp).split(\"-\"))} Test\n",
    "            \"\"\")\n",
    "        gr.ChatInterface(\n",
    "            bot_output,\n",
    "            api_name = \"chat\",\n",
    "            editable=True,\n",
    "            theme=theme,\n",
    "            type=\"messages\",\n",
    "            flagging_mode = 'manual',\n",
    "            save_history=True,\n",
    "\n",
    "            chatbot = gr.Chatbot(elem_id=\"chatbot\", \n",
    "                        height=500,\n",
    "                        type=\"messages\",\n",
    "                        placeholder=f\"<strong><br><big>JARvis</strong>\",\n",
    "                        editable=True\n",
    "                    ),\n",
    "            textbox = gr.MultimodalTextbox(\n",
    "                        interactive=True,\n",
    "                        file_count=\"multiple\",\n",
    "                        container=False,\n",
    "                        show_label=False,\n",
    "                        placeholder=\"Ask me a question\",\n",
    "                        sources=[\"microphone\", \"upload\"],\n",
    "                    ),\n",
    "        )\n",
    "        return blocks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_ui().queue().launch(\n",
    "        show_error=True, \n",
    "        show_api=True, \n",
    "        debug=True, \n",
    "        allowed_paths = [r\"Q:\\Projects\\Multimodal-Jarvis\\data\\audio\"],\n",
    "        enable_monitoring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_chat_ui():\n",
    "    blocks = gr.Blocks()\n",
    "    with blocks:\n",
    "        gr.Markdown(\n",
    "        f\"\"\"\n",
    "        # {\" \".join(os.path.basename(model_name_nlp).split(\"-\"))} Test\n",
    "        \"\"\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=10):\n",
    "                chatbot = gr.Chatbot(elem_id=\"chatbot\", \n",
    "                    height=500,\n",
    "                    type=\"messages\",\n",
    "                    bubble_full_width=False,\n",
    "                    placeholder=f\"<strong><br><big>JARvis</strong>\",\n",
    "                    editable=True\n",
    "                )\n",
    "\n",
    "                chat_input  = gr.MultimodalTextbox(\n",
    "                    interactive=True,\n",
    "                    file_count=\"multiple\",\n",
    "                    placeholder=\"Ask me a question\",\n",
    "                    container=False,\n",
    "                    scale=7,\n",
    "                    show_label=False,\n",
    "                    sources=[\"microphone\", \"upload\"],\n",
    "                )\n",
    "\n",
    "                chat_msg = chat_input.submit(\n",
    "                    add_message, \n",
    "                    [chatbot, chat_input], \n",
    "                    [chatbot, chat_input]\n",
    "                )\n",
    "                bot_msg = chat_msg.then(bot_output, chatbot, chatbot, api_name=\"bot_response\")\n",
    "                bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n",
    "\n",
    "                chatbot.like(print_like_dislike, None, None)\n",
    "                chatbot.undo(handle_undo, chatbot, [chatbot, chat_input])\n",
    "    return blocks\n",
    "\n",
    "def create_ui():\n",
    "    with gr.Blocks() as interface:\n",
    "        with gr.Tab('Chat', id='Chat', elem_id='chat-tab'):\n",
    "            create_chat_ui()\n",
    "        with gr.Tab('Chat1', id='Chat1', elem_id='tab'):\n",
    "            gr.Markdown(\"Chat1\")\n",
    "        \n",
    "    return interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_ui()\n",
    "    demo.launch(show_error=True, show_api=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 17:45:51,685 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-02-01 17:45:51,697 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
      "q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py:1017: UserWarning: Expected 1 arguments for function <function <lambda> at 0x000001F48337C310>, received 0.\n",
      "  warnings.warn(\n",
      "q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py:1021: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x000001F48337C310>, received 0.\n",
      "  warnings.warn(\n",
      "q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py:1017: UserWarning: Expected 1 arguments for function <function <lambda> at 0x000001F48337D900>, received 0.\n",
      "  warnings.warn(\n",
      "q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\utils.py:1021: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x000001F48337D900>, received 0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Block.render() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 83\u001b[0m\n\u001b[0;32m     76\u001b[0m                     btn\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m     77\u001b[0m                         fn\u001b[38;5;241m=\u001b[39mpartial(load_chat, chat_id\u001b[38;5;241m=\u001b[39mcid, chat_histories\u001b[38;5;241m=\u001b[39mchat_histories),\n\u001b[0;32m     78\u001b[0m                         inputs\u001b[38;5;241m=\u001b[39m[],  \u001b[38;5;66;03m# No additional inputs needed.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m                         outputs\u001b[38;5;241m=\u001b[39m[chatbot_display, chat_id_input]\n\u001b[0;32m     80\u001b[0m                     )\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# Place the dynamic buttons container in the layout.\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mchat_buttons_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_buttons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m demo\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "\u001b[1;31mTypeError\u001b[0m: Block.render() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 17:45:52,101 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/initiated HTTP/1.1\" 200 0\n",
      "2025-02-01 17:45:52,113 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F483752200>\n",
      "2025-02-01 17:45:52,114 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F4FFD00DC0> server_hostname='api.gradio.app' timeout=3\n",
      "2025-02-01 17:45:52,564 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F480161C30>\n",
      "2025-02-01 17:45:52,565 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-02-01 17:45:52,566 - DEBUG - send_request_headers.complete\n",
      "2025-02-01 17:45:52,568 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-02-01 17:45:52,569 - DEBUG - send_request_body.complete\n",
      "2025-02-01 17:45:52,570 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-02-01 17:45:52,782 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 01 Feb 2025 15:45:53 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "2025-02-01 17:45:52,783 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-02-01 17:45:52,785 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-02-01 17:45:52,786 - DEBUG - receive_response_body.complete\n",
      "2025-02-01 17:45:52,788 - DEBUG - response_closed.started\n",
      "2025-02-01 17:45:52,790 - DEBUG - response_closed.complete\n",
      "2025-02-01 17:45:52,792 - DEBUG - close.started\n",
      "2025-02-01 17:45:52,794 - DEBUG - close.complete\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from functools import partial\n",
    "\n",
    "def process_message(chat_histories, chat_id, message):\n",
    "    \"\"\"\n",
    "    Add the new message (and a simple bot reply) to the chat history for the provided chat_id.\n",
    "    \"\"\"\n",
    "    if chat_id not in chat_histories:\n",
    "        chat_histories[chat_id] = []\n",
    "    # For demonstration, the bot responds with the reversed message.\n",
    "    bot_response = f\"Bot: {message[::-1]}\"\n",
    "    chat_histories[chat_id].append((f\"You: {message}\", bot_response))\n",
    "    # Return updated chat histories and the conversation for the active chat.\n",
    "    return chat_histories, chat_histories[chat_id]\n",
    "\n",
    "def load_chat(chat_id, chat_histories):\n",
    "    \"\"\"\n",
    "    Return the chat history for a given chat_id.\n",
    "    \"\"\"\n",
    "    return chat_histories.get(chat_id, []), chat_id\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Chatbot with Dynamic Chat History Buttons\")\n",
    "    \n",
    "    # State variable to hold all chat histories as a dictionary:\n",
    "    #   { chat_id: [ (user message, bot response), ... ] }\n",
    "    chat_histories_state = gr.State({})\n",
    "    \n",
    "    with gr.Row():\n",
    "        chat_id_input = gr.Textbox(\n",
    "            label=\"Chat Session ID\",\n",
    "            placeholder=\"Enter a unique chat ID (e.g., user1)\"\n",
    "        )\n",
    "        user_message = gr.Textbox(\n",
    "            label=\"Your Message\",\n",
    "            placeholder=\"Type your message here\"\n",
    "        )\n",
    "        send_btn = gr.Button(\"Send\")\n",
    "    \n",
    "    # Chatbot display that shows the current conversation.\n",
    "    chatbot_display = gr.Chatbot(label=\"Conversation\")\n",
    "    \n",
    "    # A container where the dynamic chat buttons will be rendered.\n",
    "    chat_buttons_container = gr.Column()\n",
    "    \n",
    "    # When the Send button is clicked, process the new message.\n",
    "    send_btn.click(\n",
    "        process_message,\n",
    "        inputs=[chat_histories_state, chat_id_input, user_message],\n",
    "        outputs=[chat_histories_state, chatbot_display]\n",
    "    ).then(\n",
    "        # Then re-render the dynamic buttons using our render_buttons function.\n",
    "        lambda state: state,  # Pass through the state (dummy function)\n",
    "        outputs=[]  # No direct outputs here.\n",
    "    ).then(\n",
    "        # Now update the dynamic buttons.\n",
    "        lambda state: state,  # Again, pass through\n",
    "        outputs=[]  # We just trigger a state change to update our render.\n",
    "    )\n",
    "    \n",
    "    # ---\n",
    "    # The dynamic rendering function: whenever chat_histories_state changes,\n",
    "    # this function re-renders the buttons.\n",
    "    @gr.render(inputs=chat_histories_state)\n",
    "    def render_buttons(chat_histories):\n",
    "        with gr.Column() as container:\n",
    "            # If there are no chat sessions yet, display a message.\n",
    "            if not chat_histories:\n",
    "                gr.Markdown(\"### No chats available\")\n",
    "            else:\n",
    "                # For each chat session, create a button.\n",
    "                for cid in chat_histories.keys():\n",
    "                    btn = gr.Button(f\"Chat: {cid}\", key=cid)\n",
    "                    # When this button is clicked, load that chat's history.\n",
    "                    # We use functools.partial to bind the current chat ID.\n",
    "                    btn.click(\n",
    "                        fn=partial(load_chat, chat_id=cid, chat_histories=chat_histories),\n",
    "                        inputs=[],  # No additional inputs needed.\n",
    "                        outputs=[chatbot_display, chat_id_input]\n",
    "                    )\n",
    "        return container\n",
    "\n",
    "    # Place the dynamic buttons container in the layout.\n",
    "    chat_buttons_container.render(render_buttons, chat_histories_state)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:19:29,113 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-02-03 16:19:29,130 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
      "2025-02-03 16:19:29,460 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/initiated HTTP/1.1\" 200 0\n",
      "2025-02-03 16:19:29,491 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002170D00EC20>\n",
      "2025-02-03 16:19:29,533 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000021776F2DC40> server_hostname='api.gradio.app' timeout=3\n",
      "2025-02-03 16:19:29,744 - DEBUG - Using selector: SelectSelector\n",
      "2025-02-03 16:19:29,765 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None\n",
      "2025-02-03 16:19:29,767 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002170E7E5F90>\n",
      "2025-02-03 16:19:29,769 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:29,770 - DEBUG - send_request_headers.complete\n",
      "2025-02-03 16:19:29,772 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:29,773 - DEBUG - send_request_body.complete\n",
      "2025-02-03 16:19:29,773 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:29,774 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Mon, 03 Feb 2025 14:19:29 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])\n",
      "2025-02-03 16:19:29,775 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 16:19:29,776 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:29,778 - DEBUG - receive_response_body.complete\n",
      "2025-02-03 16:19:29,779 - DEBUG - response_closed.started\n",
      "2025-02-03 16:19:29,782 - DEBUG - response_closed.complete\n",
      "2025-02-03 16:19:29,783 - DEBUG - close.started\n",
      "2025-02-03 16:19:29,785 - DEBUG - close.complete\n",
      "2025-02-03 16:19:29,787 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None\n",
      "2025-02-03 16:19:29,789 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002170E802860>\n",
      "2025-02-03 16:19:29,790 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>\n",
      "2025-02-03 16:19:29,792 - DEBUG - send_request_headers.complete\n",
      "2025-02-03 16:19:29,794 - DEBUG - send_request_body.started request=<Request [b'HEAD']>\n",
      "2025-02-03 16:19:29,795 - DEBUG - send_request_body.complete\n",
      "2025-02-03 16:19:29,807 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>\n",
      "2025-02-03 16:19:29,809 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Mon, 03 Feb 2025 14:19:29 GMT'), (b'server', b'uvicorn'), (b'content-length', b'39787'), (b'content-type', b'text/html; charset=utf-8')])\n",
      "2025-02-03 16:19:29,810 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 16:19:29,811 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>\n",
      "2025-02-03 16:19:29,811 - DEBUG - receive_response_body.complete\n",
      "2025-02-03 16:19:29,813 - DEBUG - response_closed.started\n",
      "2025-02-03 16:19:29,814 - DEBUG - response_closed.complete\n",
      "2025-02-03 16:19:29,816 - DEBUG - close.started\n",
      "2025-02-03 16:19:29,817 - DEBUG - close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:19:29,825 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-02-03 16:19:29,968 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002177D2FA8C0>\n",
      "2025-02-03 16:19:29,970 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:29,970 - DEBUG - send_request_headers.complete\n",
      "2025-02-03 16:19:29,973 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:29,974 - DEBUG - send_request_body.complete\n",
      "2025-02-03 16:19:29,975 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:30,050 - DEBUG - https://huggingface.co:443 \"HEAD /api/telemetry/gradio/launched HTTP/1.1\" 200 0\n",
      "2025-02-03 16:19:30,181 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 03 Feb 2025 14:19:30 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "2025-02-03 16:19:30,182 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-02-03 16:19:30,186 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2025-02-03 16:19:30,188 - DEBUG - receive_response_body.complete\n",
      "2025-02-03 16:19:30,190 - DEBUG - response_closed.started\n",
      "2025-02-03 16:19:30,193 - DEBUG - response_closed.complete\n",
      "2025-02-03 16:19:30,196 - DEBUG - close.started\n",
      "2025-02-03 16:19:30,198 - DEBUG - close.complete\n",
      "q:\\miniAnaconda\\envs\\ml\\lib\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import inspect\n",
    "from gradio.components.multimodal_textbox import MultimodalPostprocess\n",
    "from typing import Literal, Union, cast\n",
    "import anyio\n",
    "import copy\n",
    "import dataclasses\n",
    "from gradio import utils\n",
    "from gradio.helpers import special_args\n",
    "from collections.abc import AsyncGenerator\n",
    "from gradio.components.chatbot import (\n",
    "    ChatMessage,\n",
    "    Message,\n",
    "    MessageDict,\n",
    "    TupleFormat,\n",
    ")\n",
    "multimodal = True\n",
    "limiter = None\n",
    "\n",
    "############################################################\n",
    "############### Bussines Logic #############################\n",
    "############################################################\n",
    "\n",
    "def bot_output(_unused, history: list):\n",
    "    try:\n",
    "        history.append(gr.ChatMessage(role=\"assistant\", content=\"\"))\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            history,    # [msg.dict() for msg in history]\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        model_inputs  = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        generated_text  = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        if not generated_text or not isinstance(generated_text, str):  \n",
    "            generated_text = \"I'm sorry, but I couldn't generate a response.\"\n",
    "\n",
    "        for char in generated_text :\n",
    "            history[-1].content += char\n",
    "            sleep(0.01) \n",
    "            yield history\n",
    "\n",
    "    \n",
    "        yield history\n",
    "\n",
    "    except Exception as e:\n",
    "        history.append(gr.ChatMessage(role=\"system\", content=f\"Failed to create text: {str(e)}\"))\n",
    "        yield history\n",
    "        raise gr.Error(f\"Failed to create text: {str(e)}\")\n",
    "\n",
    "fn = bot_output\n",
    "############################################################\n",
    "############### Inner Logic ################################\n",
    "############################################################\n",
    "\n",
    "def save_conversation(index: int | None, conversation: list[gr.MessageDict], saved_conversations: list[list[gr.MessageDict]]):\n",
    "    if index is not None:\n",
    "        saved_conversations[index] = conversation\n",
    "    else:\n",
    "        saved_conversations.append(conversation)\n",
    "        index = len(saved_conversations) - 1\n",
    "    return index, saved_conversations\n",
    "\n",
    "def delete_conversation(index: int | None, saved_conversations: list[list[gr.MessageDict]]):\n",
    "\n",
    "    if index is not None:\n",
    "        saved_conversations.pop(index)\n",
    "    return None, saved_conversations\n",
    "\n",
    "def generate_chat_title(conversation: list[gr.MessageDict]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a title for a conversation by taking the first user message that is a string\n",
    "        and truncating it to 40 characters. If files are present, add a üìé to the title.\n",
    "        \"\"\"\n",
    "        title = \"\"\n",
    "        for message in conversation:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                if isinstance(message[\"content\"], str):\n",
    "                    title += message[\"content\"]\n",
    "                    break\n",
    "                else:\n",
    "                    title += \"üìé \"\n",
    "        if len(title) > 40:\n",
    "            title = title[:40] + \"...\"\n",
    "        return title or \"Conversation\"\n",
    "\n",
    "def load_conversation(index: int, conversations: list[list[gr.MessageDict]]):\n",
    "    return (\n",
    "        index,\n",
    "        gr.Chatbot(\n",
    "            value=conversations[index],  # type: ignore\n",
    "            feedback_value=[],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def load_chat_history(conversations):\n",
    "        return gr.Dataset(\n",
    "            samples=[\n",
    "                [generate_chat_title(conv)]\n",
    "                for conv in conversations or []\n",
    "                if conv\n",
    "            ]\n",
    "        )\n",
    "\n",
    "def clear_and_save_textbox(\n",
    "        message: str | MultimodalPostprocess,\n",
    "    ) -> tuple[\n",
    "        gr.Textbox | gr.MultimodalTextbox,\n",
    "        str | MultimodalPostprocess,\n",
    "    ]:\n",
    "        return (\n",
    "            type(chat_input)(\"\", interactive=False, placeholder=\"\"),\n",
    "            message,\n",
    "        )\n",
    "\n",
    "@staticmethod\n",
    "def messages_to_tuples(history_messages: list[gr.MessageDict]) -> TupleFormat:\n",
    "    history_tuples = []\n",
    "    for message in history_messages:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            history_tuples.append((message[\"content\"], None))\n",
    "        elif history_tuples and history_tuples[-1][1] is None:\n",
    "            history_tuples[-1] = (history_tuples[-1][0], message[\"content\"])\n",
    "        else:\n",
    "            history_tuples.append((None, message[\"content\"]))\n",
    "    return history_tuples\n",
    "\n",
    "@staticmethod\n",
    "def tuples_to_messages(history_tuples: TupleFormat) -> list[MessageDict]:\n",
    "    history_messages = []\n",
    "    for message_tuple in history_tuples:\n",
    "        if message_tuple[0]:\n",
    "            history_messages.append({\"role\": \"user\", \"content\": message_tuple[0]})\n",
    "        if message_tuple[1]:\n",
    "            history_messages.append(\n",
    "                {\"role\": \"assistant\", \"content\": message_tuple[1]}\n",
    "            )\n",
    "    return history_messages\n",
    "\n",
    "def message_as_message_dict(\n",
    "        message: gr.MessageDict | Message | str | gr.Component | MultimodalPostprocess | list,\n",
    "        role: Literal[\"user\", \"assistant\"],\n",
    "    ) -> list[MessageDict]:\n",
    "        \"\"\"\n",
    "        Converts a user message, example message, or response from the chat function to a\n",
    "        list of MessageDict objects that can be appended to the chat history.\n",
    "        \"\"\"\n",
    "        message_dicts = []\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        for msg in message:\n",
    "            if isinstance(msg, Message):\n",
    "                message_dicts.append(msg.model_dump())\n",
    "            elif isinstance(msg, ChatMessage):\n",
    "                msg.role = role\n",
    "                message_dicts.append(\n",
    "                    dataclasses.asdict(msg, dict_factory=utils.dict_factory)\n",
    "                )\n",
    "            elif isinstance(msg, (str, gr.Component)):\n",
    "                message_dicts.append({\"role\": role, \"content\": msg})\n",
    "            elif (\n",
    "                isinstance(msg, dict) and \"content\" in msg\n",
    "            ):  # in MessageDict format already\n",
    "                msg[\"role\"] = role\n",
    "                message_dicts.append(msg)\n",
    "            else:  # in MultimodalPostprocess format\n",
    "                for x in msg.get(\"files\", []):\n",
    "                    if isinstance(x, dict):\n",
    "                        x = x.get(\"path\")\n",
    "                    message_dicts.append({\"role\": role, \"content\": (x,)})\n",
    "                if msg[\"text\"] is None or not isinstance(msg[\"text\"], str):\n",
    "                    pass\n",
    "                else:\n",
    "                    message_dicts.append({\"role\": role, \"content\": msg[\"text\"]})\n",
    "        return message_dicts\n",
    "\n",
    "def append_message_to_history(\n",
    "        message: gr.MessageDict | Message | str | gr.Component | MultimodalPostprocess | list,\n",
    "        history: list[gr.MessageDict] | TupleFormat,\n",
    "        role: Literal[\"user\", \"assistant\"] = \"user\",\n",
    "    ) -> list[gr.MessageDict] | TupleFormat:\n",
    "        message_dicts = message_as_message_dict(message, role)\n",
    "        if type == \"tuples\":\n",
    "            history = tuples_to_messages(history)  # type: ignore\n",
    "        else:\n",
    "            history = copy.deepcopy(history)\n",
    "        history.extend(message_dicts)  # type: ignore\n",
    "        if type == \"tuples\":\n",
    "            history = messages_to_tuples(history)  # type: ignore\n",
    "        return history\n",
    "\n",
    "def pop_last_user_message(\n",
    "        history: list[MessageDict] | TupleFormat,\n",
    "    ) -> tuple[list[MessageDict] | TupleFormat, str | MultimodalPostprocess]:\n",
    "        \"\"\"\n",
    "        Removes the message (or set of messages) that the user last sent from the chat history and returns them.\n",
    "        If self.multimodal is True, returns a MultimodalPostprocess (dict) object with text and files.\n",
    "        If self.multimodal is False, returns just the message text as a string.\n",
    "        \"\"\"\n",
    "        if not history:\n",
    "            return history, \"\" if not multimodal else {\"text\": \"\", \"files\": []}\n",
    "\n",
    "        if type == \"tuples\":\n",
    "            history = tuples_to_messages(history)  # type: ignore\n",
    "        i = len(history) - 1\n",
    "        while i >= 0 and history[i][\"role\"] == \"assistant\":  # type: ignore\n",
    "            i -= 1\n",
    "        while i >= 0 and history[i][\"role\"] == \"user\":  # type: ignore\n",
    "            i -= 1\n",
    "        last_messages = history[i + 1 :]\n",
    "        last_user_message = \"\"\n",
    "        files = []\n",
    "        for msg in last_messages:\n",
    "            assert isinstance(msg, dict)  # noqa: S101\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                content = msg[\"content\"]\n",
    "                if isinstance(content, tuple):\n",
    "                    files.append(content[0])\n",
    "                else:\n",
    "                    last_user_message = content\n",
    "        return_message = (\n",
    "            {\"text\": last_user_message, \"files\": files}\n",
    "            if multimodal\n",
    "            else last_user_message\n",
    "        )\n",
    "        history_ = history[: i + 1]\n",
    "        if type == \"tuples\":\n",
    "            history_ = messages_to_tuples(history_)  # type: ignore\n",
    "        return history_, return_message  # type: ignore\n",
    "\n",
    "\n",
    "async def submit_fn(message: str | MultimodalPostprocess, history: TupleFormat | list[MessageDict], request: gr.Request, *args,\n",
    ") -> tuple:\n",
    "    inputs, _, _ = special_args(\n",
    "        fn, inputs=[message, history, *args], request=request\n",
    "    )\n",
    "    if is_async:\n",
    "        response = await fn(*inputs)\n",
    "    else:\n",
    "        response = await anyio.to_thread.run_sync(\n",
    "            fn, *inputs, limiter=limiter\n",
    "        )\n",
    "    if additional_outputs:\n",
    "        response, *additional_outputs = response\n",
    "    else:\n",
    "        additional_outputs = None\n",
    "    history = append_message_to_history(message, history, \"user\")\n",
    "    history = append_message_to_history(response, history, \"assistant\")\n",
    "    if additional_outputs:\n",
    "        return response, history, *additional_outputs\n",
    "    return response, history\n",
    "\n",
    "async def stream_fn(message: str | MultimodalPostprocess, history: TupleFormat | list[MessageDict], request: gr.Request, *args,\n",
    ") -> AsyncGenerator[\n",
    "    tuple,\n",
    "    None,\n",
    "]:\n",
    "    inputs, _, _ = special_args(\n",
    "        fn, inputs=[message, history, *args], request=request\n",
    "    )\n",
    "    if is_async:\n",
    "        generator = fn(*inputs)\n",
    "    else:\n",
    "        generator = await anyio.to_thread.run_sync(\n",
    "            fn, *inputs, limiter=limiter\n",
    "        )\n",
    "        generator = utils.SyncToAsyncIterator(generator, limiter)\n",
    "\n",
    "    history = append_message_to_history(message, history, \"user\")\n",
    "    additional_outputs = None\n",
    "    try:\n",
    "        first_response = await utils.async_iteration(generator)\n",
    "        if additional_outputs:\n",
    "            first_response, *additional_outputs = first_response\n",
    "        history_ = append_message_to_history(\n",
    "            first_response, history, \"assistant\"\n",
    "        )\n",
    "        if not additional_outputs:\n",
    "            yield first_response, history_\n",
    "        else:\n",
    "            yield first_response, history_, *additional_outputs\n",
    "    except StopIteration:\n",
    "        yield None, history\n",
    "    async for response in generator:\n",
    "        if additional_outputs:\n",
    "            response, *additional_outputs = response\n",
    "        history_ = append_message_to_history(response, history, \"assistant\")\n",
    "        if not additional_outputs:\n",
    "            yield response, history_\n",
    "        else:\n",
    "            yield response, history_, *additional_outputs\n",
    "\n",
    "def edit_message(history: list[MessageDict] | TupleFormat, edit_data: gr.EditData) -> tuple[\n",
    "        list[MessageDict] | TupleFormat,\n",
    "        list[MessageDict] | TupleFormat,\n",
    "        str | MultimodalPostprocess,\n",
    "    ]:\n",
    "        if isinstance(edit_data.index, (list, tuple)):\n",
    "            history = history[: edit_data.index[0]]\n",
    "        else:\n",
    "            history = history[: edit_data.index]\n",
    "        return history, history, edit_data.value\n",
    "\n",
    "############################################################\n",
    "############### UI Logic from ChatInterface ################\n",
    "############################################################\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            \n",
    "            new_chat_button = gr.Button(\n",
    "                \"New chat\",\n",
    "                variant=\"primary\",\n",
    "                size=\"md\",\n",
    "            )\n",
    "            chat_history_dataset = gr.Dataset(\n",
    "                components=[gr.Textbox(visible=False)],\n",
    "                show_label=False,\n",
    "                layout=\"table\",\n",
    "                type=\"index\",\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=5):\n",
    "            chatbot = gr.Chatbot(elem_id=\"chatbot\", \n",
    "                height=500,\n",
    "                type=\"messages\",\n",
    "                placeholder=f\"<strong><br><big>JARvis</strong>\",\n",
    "                editable=True\n",
    "            )\n",
    "            chat_input = gr.MultimodalTextbox(\n",
    "                interactive=True,\n",
    "                file_count=\"multiple\",\n",
    "                container=False,\n",
    "                show_label=False,\n",
    "                placeholder=\"Ask me a question\",\n",
    "                sources=[\"microphone\", \"upload\"],\n",
    "            )\n",
    "\n",
    "        chatbot_state = gr.State(chatbot.value if chatbot.value else [])\n",
    "        chatbot_value = gr.State(chatbot.value if chatbot.value else [])\n",
    "        null_component = gr.State()\n",
    "        is_generator = inspect.isgeneratorfunction(\n",
    "            fn\n",
    "        ) or inspect.isasyncgenfunction(fn)\n",
    "        is_async = inspect.iscoroutinefunction(\n",
    "            fn\n",
    "        ) or inspect.isasyncgenfunction(fn)\n",
    "        submit_fn = stream_fn if is_generator else submit_fn\n",
    "\n",
    "        saved_conversations = gr.BrowserState(\n",
    "                [], storage_key=f\"saved_conversations_{id}\"\n",
    "            )\n",
    "        conversation_id = gr.State(None)\n",
    "        saved_input = gr.State() \n",
    "        synchronize_chat_state_kwargs = {\n",
    "                    \"fn\": lambda x: (x, x),\n",
    "                    \"inputs\": [chatbot],\n",
    "                    \"outputs\": [chatbot_state, chatbot_value],\n",
    "                    \"show_api\": False,\n",
    "                    \"queue\": False,\n",
    "                }\n",
    "        save_fn_kwargs = {\n",
    "            \"fn\": save_conversation,\n",
    "            \"inputs\": [\n",
    "                conversation_id,\n",
    "                chatbot_state,\n",
    "                saved_conversations,\n",
    "            ],\n",
    "            \"outputs\": [conversation_id, saved_conversations],\n",
    "            \"show_api\": False,\n",
    "            \"queue\": False,\n",
    "        }\n",
    "        submit_fn_kwargs = {\n",
    "            \"fn\": submit_fn,\n",
    "            \"inputs\": [saved_input, chatbot_state],\n",
    "            \"outputs\": [null_component, chatbot],\n",
    "            \"show_api\": False,\n",
    "            \"concurrency_limit\": cast(\n",
    "                Union[int, Literal[\"default\"], None], 'default'\n",
    "            ),\n",
    "            \"show_progress\": cast(\n",
    "                Literal[\"full\", \"minimal\", \"hidden\"], 'full'\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "        submit_event = (\n",
    "        chat_input.submit( \n",
    "            clear_and_save_textbox,\n",
    "            [chat_input],\n",
    "            [chat_input, saved_input],\n",
    "            show_api=False,\n",
    "            queue=False,\n",
    "        ).then(  # The reason we do this outside of the submit_fn is that we want to update the chatbot UI with the user message immediately, before the submit_fn is called\n",
    "                append_message_to_history,\n",
    "                [saved_input, chatbot],\n",
    "                [chatbot],\n",
    "                show_api=False,\n",
    "                queue=False,\n",
    "            ).then(**submit_fn_kwargs)\n",
    "        )\n",
    "        submit_event.then(**synchronize_chat_state_kwargs).then(\n",
    "            lambda: gr.update(value=None, interactive=True),\n",
    "            None,\n",
    "            chat_input,\n",
    "            show_api=False,\n",
    "        ).then(**save_fn_kwargs)\n",
    "\n",
    "        retry_event = (\n",
    "            chatbot.retry(\n",
    "                pop_last_user_message,\n",
    "                [chatbot_state],\n",
    "                [chatbot_state, saved_input],\n",
    "                show_api=False,\n",
    "                queue=False,\n",
    "            )\n",
    "            .then(\n",
    "                append_message_to_history,\n",
    "                [saved_input, chatbot_state],\n",
    "                [chatbot],\n",
    "                show_api=False,\n",
    "                queue=False,\n",
    "            )\n",
    "            .then(\n",
    "                lambda: gr.update(interactive=False, placeholder=\"\"),\n",
    "                outputs=[chat_input],\n",
    "                show_api=False,\n",
    "            ).then(**submit_fn_kwargs)\n",
    "        )\n",
    "        retry_event.then(**synchronize_chat_state_kwargs).then(\n",
    "            lambda: gr.update(interactive=True),\n",
    "            outputs=[chat_input],\n",
    "            show_api=False,\n",
    "        ).then(**save_fn_kwargs)\n",
    "\n",
    "\n",
    "        chatbot.undo(\n",
    "            pop_last_user_message,\n",
    "            [chatbot],\n",
    "            [chatbot, chat_input],\n",
    "            show_api=False,\n",
    "            queue=False,\n",
    "        ).then(**synchronize_chat_state_kwargs).then(**save_fn_kwargs)\n",
    "\n",
    "        chatbot.clear(**synchronize_chat_state_kwargs).then(\n",
    "            delete_conversation,\n",
    "            [conversation_id, saved_conversations],\n",
    "            [conversation_id, saved_conversations],\n",
    "            show_api=False,\n",
    "            queue=False,\n",
    "        )\n",
    "\n",
    "        new_chat_button.click(\n",
    "            lambda: (None, []),\n",
    "            None,\n",
    "            [conversation_id, chatbot],\n",
    "            show_api=False,\n",
    "            queue=False,\n",
    "        ).then(\n",
    "            lambda x: x,\n",
    "            [chatbot],\n",
    "            [chatbot_state],\n",
    "            show_api=False,\n",
    "            queue=False,\n",
    "        )\n",
    "\n",
    "        saved_conversations.change(\n",
    "            fn=load_chat_history,\n",
    "            inputs=[saved_conversations],\n",
    "            outputs=[chat_history_dataset],\n",
    "            show_api=False,\n",
    "            queue=False,\n",
    "        )\n",
    "\n",
    "        chat_history_dataset.click(\n",
    "            lambda: [],\n",
    "            None,\n",
    "            [chatbot],\n",
    "            show_api=False,\n",
    "            queue=False,\n",
    "            show_progress=\"hidden\",\n",
    "        ).then(\n",
    "            load_conversation,\n",
    "            [chat_history_dataset, saved_conversations],\n",
    "            [conversation_id, chatbot],\n",
    "            show_api=False,\n",
    "            queue=False,\n",
    "            show_progress=\"hidden\",\n",
    "        ).then(**synchronize_chat_state_kwargs)\n",
    "\n",
    "        if chatbot.editable:\n",
    "            chatbot.edit(\n",
    "                edit_message,\n",
    "                [chatbot],\n",
    "                [chatbot, chatbot_state, saved_input],\n",
    "                show_api=False,\n",
    "            ).success(**submit_fn_kwargs).success(**synchronize_chat_state_kwargs).then(\n",
    "                **save_fn_kwargs\n",
    "            )\n",
    "\n",
    "demo.launch(show_error=True, show_api=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation from text-generation-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "css = r\"src/static/static.css\"\n",
    "\n",
    "sidebar_html = r\"src/templates/main.html\"\n",
    "\n",
    "def create_chat_ui():\n",
    "    with gr.Blocks() as blocks:\n",
    "      gr.Chatbot(elem_id=\"chatbot\", \n",
    "          height=620,\n",
    "          type=\"messages\",\n",
    "          bubble_full_width=False,\n",
    "          placeholder=f\"<strong><br><big>JARvis</br></strong>\",\n",
    "          editable=True\n",
    "      )\n",
    "      gr.MultimodalTextbox(\n",
    "          interactive=True,\n",
    "          file_count=\"multiple\",\n",
    "          placeholder=\"Ask me a question\",\n",
    "          container=False,\n",
    "          scale=7,\n",
    "          show_label=False,\n",
    "          sources=[\"microphone\", \"upload\"],\n",
    "      )\n",
    "    return blocks\n",
    "\n",
    "def create_setting_ui():\n",
    "    with gr.Blocks() as blocks:\n",
    "       gr.Slider(minimum=0, maximum=100, label=\"Volume\")\n",
    "       gr.Slider(minimum=0, maximum=100, label=\"Brightness\")\n",
    "       gr.Slider(minimum=0, maximum=100, label=\"Contrast\")\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def create_interface():\n",
    "  with gr.Blocks(css=css, theme=theme) as demo:\n",
    "    with gr.Row():\n",
    "      gr.HTML(sidebar_html)\n",
    "\n",
    "      with gr.Column(scale=8, elem_classes=\"main-content\"):\n",
    "        \n",
    "        with gr.Group(visible=True) as chat_group:\n",
    "          create_chat_ui()\n",
    "        \n",
    "        with gr.Group(visible=False) as settings_group:\n",
    "          create_setting_ui()\n",
    "\n",
    "      return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_interface()\n",
    "    demo.launch(show_error=True, show_api=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
